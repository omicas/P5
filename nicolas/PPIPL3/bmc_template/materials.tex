\subsection*{Data Availability}

The human interactome data was downloaded from the repository of the
length-3 degree normalized paths methodology \cite{Kovacs2019}: the
dataset \emph{HI-II-14} and \emph{HI-TESTED} are used for prediction.
The dataset \emph{HI-III} is available in the same repository and
is used for validation.

The rice interactome information was downloaded from the STRING database
\cite{Szklarczyk2019}, corresponding to the subspecies \emph{Oryza
sativa}. The file contains more than 8 million PPIs from several sources.
For the purpose of this study, only PPIs with evidence from curated
databases are used. The resulting network contains $5025$ nodes and
$164420$ edges.

\subsection*{Code Implementation}

The original code implementation in \texttt{C++} from Kovacs et al
(2019) was adapted to \texttt{Python} (V3.6). For the purpose of algorithmic
validation, the three methods were implemented from scratch with basic
functionalities and data structures of the \texttt{Python} language.
The code is publicly available under the repository github.com/ocinlr/PPIPL3.

\subsection*{Data Preprocessing}

Information for the human interactome was used as-is, which corresponds
to networks of $4298$, $3727$ and $5604$ proteins and $13868$,
$9433$ and $23322$ interactions, respectively.

For the rice interactome, an additional preprocessing was performed.
The filtered network for rice consists of $5025$ proteins (nodes)
and $164420$ interactions (edges) distributed among $178$ connected
components. The connected component with the greatest number of edges
was selected. The extracted connected component consists of $n=4390$
nodes and $m=163319$ edges, which corresponds to $99.33$\% of edges.

\subsection*{Edge Prediction}

For predicting interactions for each network, the algorithms described
below were used. It is important to keep in mind how the PPI network
$G=(V,E)$ is conceptualized: each node ($v_{i}\in V$) represents
a protein and each undirected edge ($e_{b}=\{v_{i},v_{j}\},\,e_{b}\in E$)
represents an interaction among proteins $v_{i}$ and $v_{j}$.
%\begin{description}\paragraph*{Common~Neighbors~(A2)} 
%\item [{Common~Neighbors~(A2)}] :This method is based on the Triadic Closure
\paragraph*{Common~Neighbors~(A2):} This method is based on the Triadic Closure
Principle which states that the more common friends two individuals
have, the more likely that they know each other. For implementing
this measure, $A{{}^2}$ matrix is calculated, where $A$ is the adjacency
matrix of the network.
%\item [{Length-3~Path~Counts~(A3)}] This is a simple implementation
\paragraph*{Length-3~Path~Counts~(A3):} This is a simple implementation
of the notion of ``if my friends and your friends interact, then
we might interact too''. For implementing this measure, $A{{}^3}$
is calculated.
%\item [{Degree-normalized~L-3~Score~(L3)}] The previous approach might
\paragraph*{Degree-normalized~L-3~Score~(L3):} The previous approach might
overestimate the importance of some edges due to intermediate hubs
which add many shortcuts in the graph. To address that issue, a degree
normalization for the path $v_{i}\rightarrow v_{m}\rightarrow v_{n}\rightarrow v_{j}$
is applied by considering the degree of the intermediate nodes $k_{m}$
and $k_{n}$, as follows.
\[
p_{i,j}=\sum_{m,n\in|V|}\frac{A_{i,m}\cdot A_{m,n}\cdot A_{n,j}}{\sqrt{k_{m}\cdot k_{n}}}
\]
\\
where $A_{i,j}$ represents the value of the adjacency matrix for
nodes $i$ and $j$.
%\end{description}

\subsection*{Sampling Procedure}

For each PPI network, the following procedure was performed 10 times
to address the stochastic nature of the process:
\begin{enumerate}
\item A percentage of interactions is removed at random from the network
(20\%).
\item The same amount of removed interactions are then predicted using the
main methods for prediction mentioned by Kovacs et al (2019): Common
Neighbors (\textbf{A2}), raw path count of paths of length 3 (\textbf{A3})
and the Length-3 degree-normalized score (\textbf{L3}).
\item A test dataset is created as follows: all removed edges are included
(as observed positives for the ML algorithm) and from the predicted
edges of \textbf{A2}, \textbf{A3} and \textbf{L3} that don't lie in
the previous classification (observed negatives), a random subset
is chosen such that the dataset is balanced, that is, the amount of
observed positive labels is equal to the observed negative labels.
\item Once the dataset is ready, it is randomly partitioned: 80\% is used
for \texttt{XGBoost} model training and 20\% is used for validation.
It is important to have in mind that balanced distribution of the
positive and negative labels in the datasets was satisfied in training,
but validation is performed using the remaining unbalanced data.
\end{enumerate}
It is worth mentioning that some exploration experiments were carried
out with interactions removal percentages of 2, 5, 10 and 20\%, as
well as train-test partitions of 15-85, 80-20 and 75-25, and the explained
parameter set was selected because it either represented a marginal
gain (results not shown) or because it was a common parameter selection
in related literature.

\subsection*{Feature Extraction with \texttt{Node2Vec}}

The \texttt{Node2Vec} module was used for extracting features of the
interactomes. The considerations for the model were:
\begin{itemize}
\item All paths in the random walks are equally likely.
\item Use a modest number of dimensions and threads for calculation.
\item Since length-3 paths are the defining property in this study, there
is no necessity for longer walks. However, it is important to try
out many possible redundant routes and to consider a window of at
least 4.
\item Other standard parameters were left with default values.
\item Edge embeddings were calculated using a geometric ratio of the node
embeddings.
\end{itemize}

\subsection*{Handcrafted Features}

Due to the poor results of the \emph{raw} Length-3 counting (\textbf{A3}),
a different approach for this information was carried out: As it still
gives a lot of information that might be useful for a predictive routine,
this counting was normalized (dividing by the greatest counting in
the \textbf{A3} top predictions) and then used as a feature for the
Machine Learning algorithm. For completeness, \textbf{A2} and \textbf{L3}
measures were used as a input features. Finally, the case were no
handcrafted feature was also considered, that is, only the features
extracted from the structure of the network were considered.

\subsection*{Feature to Predict: Existence}

The feature to predict corresponds to the possible existence (\emph{True/False})
of a link based on the existing information of the network. This is
evaluated by taking out a fraction of the edges and then trying to
predict for a given set of possible edges if they have a high probability
to belong to the original network.

\subsection*{Machine Learning Algorithm}

The Extreme Gradient Boosting implementation of gradient boosted trees
is applied to evaluate the existence of an edge. Gradient boosted
trees are usually used for supervised learning problems, where the
training data $X_{i}$ has multiple features and pretends to explain
(or predict) a target variable $Y_{i}$. The corresponding implementation
applied for this study is \texttt{XGBoost}.

\subsection*{Result Validation}

As mentioned before, 80\% of the final dataset was randomly selected
and used for training, while the remaining information was used for
validation. The whole training-validation procedure was applied 10
times.

The chosen metric for validation was the Area under the Curve (\textbf{AUC})
of the Receiver Operating Characteristic (\textbf{ROC}). This curve
corresponds to plot the sensitivity (probability of predicting a real
positive as positive) against 1-specificity (probability of predicting
a real negative as positive). AUC values of 1 represent a perfect
prediction and 0.5 corresponds to a random guess. Normally, values
over 0.8 of AUC are considered good.
