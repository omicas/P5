% "Preliminaries" refers to fundaments for the new reader on the main topics of
% the paper
\subsection*{Protein Interactions Networks}
Let $G=(V,E)$ be an undirected graph describing $n=|V|$ proteins as 
nodes and $m=|E|$ pairwise interactions between the graph as edges. 
$G$ can be represented as a Boolean adjacency matrix $A_G$ of size 
$n^2$, where the value of the entry $(i,j), 0\leq i,j < n$ is set 
to $1$ if there is a direct interaction between proteins $i$ and $j$ 
and set to $0$ otherwise. As it is an undirected network, $A_G$ is symmetric.

Given this matrix $A$, to find the number of paths of exactly $k$ steps 
in the network, $A$ can be raised to the \textit{k}-th power.

\subsection*{Feature Learning of a Network and node2vec}
The problem of feature learning in a network can be addressed as a maximum 
likelihood optimization problem, in which the function (to be learned) 
$f : V \rightarrow \mathbb{R}^d$ maps from nodes to d-dimensional feature 
representations. $f$ can also be seen as a matrix of size $|V| \times d$. 
The optimization problem is complex but tractable when assuming conditional 
independence and spatial symmetry in feature space. Namely, conditional 
independence means that the likelihood of observing a neighbor node does 
not depend on any other neighbor node.

For every node $u \in V$, there exists a network neighborhood, denoted as 
$N_S (u) \subset V$, generated by a sampling strategy limited to k to be 
able to compare different strategies fairly. When sampling, one possible strategy is to retrieve nodes as close as possible from the source. This strategy is known as Breadth-first Sampling (BFS). On the other hand, one can move as far as possible from the source, also known as Depth-first Sampling (DFS). In this regard, node2vec generates random walks from each node with user-defined weighting for back-edges (return parameter p) and for edges that go away from source (in-out parameter q). Based on the information (paths) gathered from these walks, node2vec uses a semi-supervised method to learn rich feature representations for nodes in a network.

Since the purpose of this study is link prediction, another mapping function is defined for the edges, so that its feature representation is a bootstrap over the feature representation of nodes. Let $g : V \times V \rightarrow \mathbb{R}^d$, then the function $g$ is defined for any pair of nodes $(u,v)$, even if this edge does not exist in the network. For this work, the Hadamard operator is used, which consists of an element-wise product of the node features.

\subsection*{Gradient Boosted Trees and XGBoost}
(THIS IS PENDING)